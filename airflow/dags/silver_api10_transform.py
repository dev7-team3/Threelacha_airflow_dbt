from datetime import datetime
import json
import logging
from typing import Any, Optional

from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from connection_utils import get_storage_conn_id
import pandas as pd
from preprocessing import add_date_features, fix_cd_columns, normalize_price, prepare_metadata, upload_parquet_to_s3


@dag(
    dag_id="silver_api10_transform",
    schedule=None,
    start_date=datetime(2025, 12, 1),
    catchup=False,
    tags=["preprocessing", "api10"],
)
def silver_api10_transform():
    """
    Silver layer API10 ë°ì´í„° ì²˜ë¦¬ DAG ì •ì˜ í•¨ìˆ˜.

    ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ TaskFlow APIë¡œ êµ¬ì„±

    task 1: [extract_json] s3ì—ì„œ raw ë°ì´í„° ì¶”ì¶œ
    task 2: [transform]raw ë°ì´í„° ì „ì²˜ë¦¬
    task 3: [join_metadata] metadataì™€ joiní•˜ì—¬ ë©”íƒ€ì •ë³´ merge
    task 4: [upload] s3ì˜ silver/api10/ìœ¼ë¡œ ì—…ë¡œë“œ

    Returns:
        None
    """
    s3_conn_id = get_storage_conn_id()
    bucket = "team3-batch"
    meta_key = "metadata/dim_product_no.csv"

    @task
    def extract_json() -> pd.DataFrame:
        """
        S3(MinIO)ì—ì„œ ì‹¤í–‰ì¼ì(dt=YYYYMMDD)ì— í•´ë‹¹í•˜ëŠ” data.json íŒŒì¼ë“¤ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë³€í™˜í•œë‹¤.

        Returns:
            pd.DataFrame: ì¶”ì¶œëœ ë°ì´í„°í”„ë ˆì„.
        """
        hook = S3Hook(aws_conn_id=s3_conn_id)
        records = []

        def safe_value(val: Any) -> Optional[Any]:
            """ë¦¬ìŠ¤íŠ¸([])ëŠ” Noneìœ¼ë¡œ ë³€í™˜, ë¦¬ìŠ¤íŠ¸ì— ê°’ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©"""
            if isinstance(val, list):
                return None if len(val) == 0 else val[0]
            return val

        # ëª¨ë“  í‚¤ ê°€ì ¸ì˜¤ê¸°
        all_keys = hook.list_keys(bucket_name=bucket, prefix="raw/api-10/")
        dt_values = []

        # í‚¤ì—ì„œ dt=YYYYMMDD ì¶”ì¶œ
        dt_values = [p.replace("dt=", "") for key in all_keys for p in key.split("/") if p.startswith("dt=")]

        if not dt_values:
            raise ValueError("S3ì— dt=YYYYMMDD í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ê°€ì¥ ìµœì‹  ë‚ ì§œ ì„ íƒ
        latest_dt = max(dt_values)
        prefix = f"raw/api-10/dt={latest_dt}/product_cls=01/"

        for obj in hook.list_keys(bucket_name=bucket, prefix=prefix):
            if obj.endswith("data.json"):
                body = hook.read_key(key=obj, bucket_name=bucket)
                try:
                    data = json.loads(body)
                    price_data = data.get("price", [])
                    if isinstance(price_data, list):
                        for item in price_data:
                            record = {
                                k: safe_value(v)
                                for k, v in {
                                    "key": obj,
                                    "error_code": data.get("error_code"),
                                    "condition": data.get("condition"),
                                    **item,
                                }.items()
                            }
                            records.append(record)
                    else:
                        record = {
                            "key": obj,
                            "error_code": data.get("error_code"),
                            "condition": data.get("condition"),
                            "price": safe_value(price_data),
                        }
                        records.append(record)
                except Exception:
                    logging.exception(f"[extract_json] JSON íŒŒì‹± ì‹¤íŒ¨: {obj}")

        df = pd.DataFrame(records)
        logging.info(f"ğŸ¤–[extract_json] â†’ ì´ {len(df)}í–‰")
        return df

    @task
    def transform(df: pd.DataFrame) -> dict:
        """
        DataFrame ì „ì²˜ë¦¬ ë° ë¶„í•  (main, livestock, grocery).

        Args:
            df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„.

        Returns:
            dict: main, livestock, grocery ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬.
        """
        logging.info(f"ğŸ¤–[transform] ì´ˆê¸° shape={df.shape}")

        # í’ˆëª©ëª… ë¶„ë¦¬
        df[["item_nm", "kind_nm"]] = df["productName"].str.split("/", n=1, expand=True)

        # ê°€ê²© ì •ê·œí™”
        for col in ["dpr1", "dpr2", "dpr3", "dpr4"]:
            df[col] = normalize_price(df[col])

        # ë‚ ì§œ íŒŒìƒ ì»¬ëŸ¼ ì¶”ê°€
        df = add_date_features(df, "lastest_day")

        logging.info(
            f"ğŸ¤–[transform] 1ì°¨ ì „ì²˜ë¦¬ í™•ì¸: {df[['item_nm', 'kind_nm', 'dpr1', 'dpr3', 'lastest_day']].head(2)}"
        )

        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        df = df.rename(
            columns={
                "product_cls_code": "product_cls_cd",
                "product_cls_name": "product_cls_nm",
                "county_code": "country_cd",
                "county_name": "country_nm",
                "category_code": "category_cd",
                "category_name": "category_nm",
                "productno": "product_no",
                "lastest_day": "res_dt",
                "unit": "product_cls_unit",
                "day1": "base_dt",
                "dpr1": "base_pr",
                "day2": "prev_1d_dt",
                "dpr2": "prev_1d_pr",
                "day3": "prev_1m_dt",
                "dpr3": "prev_1m_pr",
                "day4": "prev_1y_dt",
                "dpr4": "prev_1y_pr",
                "direction": "direction_tp",
                "value": "direction_pct",
            }
        )

        # ë°ì´í„° íƒ€ì… ì²˜ë¦¬
        df["product_no"] = df["product_no"].astype("Int64")
        df = fix_cd_columns(df)

        # í•„í„°ë§
        df = df[df["product_cls_cd"] == "01"]
        logging.info(f"ğŸ¤–[transform] í•„í„°ë§ í›„ shape={df.shape}")

        # ë¶„í• 
        main_df = df[df["category_cd"].isin(["100", "200", "300", "400", "600"])]
        livestock_df = df[df["category_cd"] == "500"]
        grocery_df = df[df["category_cd"] == "800"]
        logging.info(f"ğŸ¤–[transform] main={main_df.shape}, livestock={livestock_df.shape}, grocery={grocery_df.shape}")

        return {"main": main_df, "livestock": livestock_df, "grocery": grocery_df}

    @task
    def join_metadata(dfs: dict) -> dict:
        """
        ë©”íƒ€ë°ì´í„° ì¡°ì¸ (main_dfë§Œ), ë‚˜ë¨¸ì§€ëŠ” ì»¬ëŸ¼ ìˆœì„œë§Œ ë§ì¶¤.

        Args:
            dfs (dict): main, livestock, grocery ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬.

        Returns:
            dict: ë©”íƒ€ë°ì´í„° ì¡°ì¸ í›„ ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬.
        """
        main_df = dfs["main"]
        livestock_df = dfs["livestock"]
        grocery_df = dfs["grocery"]

        hook = S3Hook(aws_conn_id=s3_conn_id)
        meta_body = hook.read_key(key=meta_key, bucket_name=bucket)
        meta_df = prepare_metadata(meta_body)

        meta_df = meta_df[["product_no", "item_cd", "kind_cd", "rank_cd", "rank_nm"]]
        meta_df.loc[len(meta_df)] = [3021, "616", "01", "21", "ä¸­"]

        merged = pd.merge(main_df, meta_df, on=["product_no"], how="left")

        desired_order = [
            # í’ˆëª© ì •ë³´
            "product_no",
            "product_cls_cd",
            "product_cls_nm",
            "category_cd",
            "category_nm",
            "item_cd",
            "item_nm",
            "kind_cd",
            "kind_nm",
            "product_cls_unit",
            # ê°€ê²© ì •ë³´
            "base_dt",
            "base_pr",
            "prev_1d_dt",
            "prev_1d_pr",
            "direction_tp",
            "direction_pct",
            "prev_1m_dt",
            "prev_1m_pr",
            "prev_1y_dt",
            "prev_1y_pr",
            # ì§€ì—­ ì •ë³´
            "country_cd",
            "country_nm",
            # ë‚ ì§œ ì •ë³´
            "res_dt",
            "year",
            "month",
            "week_of_year",
            "weekday_num",
            "weekday_nm",
            "weekend_yn",
        ]

        merged = merged[[c for c in desired_order if c in merged.columns]]
        livestock_df = livestock_df[[c for c in desired_order if c in livestock_df.columns]]
        grocery_df = grocery_df[[c for c in desired_order if c in grocery_df.columns]]

        logging.info("[join_metadata] ë©”íƒ€ë°ì´í„° ì¡°ì¸ ì™„ë£Œ")
        return {"main": merged, "livestock": livestock_df, "grocery": grocery_df}

    @task
    def upload(dfs: dict) -> None:
        """
        ìµœì¢… DataFrameë“¤ì„ Parquetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ S3ì— ì—…ë¡œë“œí•œë‹¤.

        Args:
            dfs (dict): main, livestock, grocery ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬.

        Returns:
            None
        """
        for name, df in dfs.items():
            base_prefix = f"silver/api-10/{name}"
            upload_parquet_to_s3(
                df=df,
                bucket=bucket,
                base_prefix=base_prefix,
                aws_conn_id=s3_conn_id,
            )
            logging.info(f"[upload] {name} ì—…ë¡œë“œ ì™„ë£Œ")

    # DAG ì‹¤í–‰ íë¦„
    raw_df = extract_json()
    dfs = transform(raw_df)
    dfs_final = join_metadata(dfs)
    upload(dfs_final)


silver_api10_transform()
