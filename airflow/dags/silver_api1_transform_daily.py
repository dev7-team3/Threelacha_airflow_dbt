from datetime import datetime
from io import BytesIO
import json
import logging
import re
from typing import Any, Dict, List, Optional

from airflow.exceptions import AirflowSkipException
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.sdk import dag, task
from connection_utils import get_storage_conn_id
import numpy as np
import pandas as pd
import pendulum

logger = logging.getLogger("airflow.task")

# ---------------------------------------------------------
# ìƒìˆ˜ ì •ì˜
# ---------------------------------------------------------
BUCKET_NAME = "team3-batch"
CONN_ID = get_storage_conn_id()

# ---------------------------------------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------------------------


def extract_metadata_from_path(file_path: str) -> Dict[str, str]:
    """íŒŒì¼ ê²½ë¡œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    Args:
        file_path: S3 íŒŒì¼ ê²½ë¡œ (ì˜ˆ: raw/api-1/dt=2025-01-01/product_cls=01/country=1101/category=100/data.json)

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (product_cls_cd, category_cd, country_cd, res_dt)
    """
    product_cls = re.search(r"product_cls=(\w+)", file_path)
    category = re.search(r"category=(\w+)", file_path)
    country = re.search(r"country=(\w+)", file_path)
    regday = re.search(r"dt=(\d{4}-\d{2}-\d{2})", file_path)

    return {
        "product_cls_cd": product_cls.group(1) if product_cls else "",
        "category_cd": category.group(1) if category else "",
        "country_cd": country.group(1) if country else "",
        "res_dt": regday.group(1) if regday else "",
    }


def clean_price(value: Any) -> Optional[float]:
    """ê°€ê²© ë°ì´í„° ì •ì œ (ì‰¼í‘œ ì œê±°, ë¹ˆ ê°’/'-' ì²˜ë¦¬)

    Args:
        value: ì›ë³¸ ê°€ê²© ê°’ (ì˜ˆ: "5,500", "-", "", None)

    Returns:
        ì •ì œëœ ê°€ê²© floatí˜• (ì˜ˆ: 5500.0) ë˜ëŠ” None
    """
    if pd.isna(value) or not str(value).strip() or str(value).strip() == "-":
        return None
    try:
        return float(str(value).strip().replace(",", ""))  # "5,500" -> 5500.0
    except ValueError:
        return None


# ---------------------------------------------------------
# DAG ì •ì˜
# ---------------------------------------------------------


@dag(
    dag_id="silver_api1_transform_daily",
    schedule="0 1 * * *",
    start_date=pendulum.datetime(2025, 12, 11, tz="UTC"),
    catchup=True,
    max_active_runs=10,
    default_args={
        "owner": "jungeun_park",
        "retries": 2,
        "retry_delay": pendulum.duration(minutes=5),
    },
    tags=["KAMIS", "api-1", "silver", "transform"],
    description="KAMIS API1 Raw ë°ì´í„°ë¥¼ ì½ì–´ ì½”ë“œ ë§¤í•‘ ë° íŒŒìƒ ì»¬ëŸ¼ì„ ì¶”ê°€í•œ Silver ë°ì´í„°ë¡œ ë³€í™˜",
)
def transform_api1_raw_to_silver() -> None:
    """
    KAMIS API1 Raw â†’ Silver ë³€í™˜ DAG

    ì²˜ë¦¬ íë¦„:
    1. Raw ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (S3)
    2. JSON íŒŒì¼ ì½ê¸° ë° íŒŒì‹±
    3. ì½”ë“œ ë§¤í•‘ ë° íŒŒìƒ ì»¬ëŸ¼ ìƒì„± (ìš”ì¼, ì£¼ì°¨ ë“±)
    4. Parquet í˜•ì‹ìœ¼ë¡œ Silver ë ˆì´ì–´ì— ì €ì¥ (ë‚ ì§œë³„ íŒŒì¼)

    Note:
    - day1~day7 ì»¬ëŸ¼ì€ "ë‹¹ì¼ (12/11)", "1ì¼ì „ (12/10)" ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥
    - ì‹¤ì œ ë‚ ì§œ ê¸°ì¤€ì€ res_dt ì»¬ëŸ¼ ì‚¬ìš©
    """

    @task
    def list_raw_files(target_date: str) -> List[str]:
        """S3ì—ì„œ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ

        Args:
            target_date: ì²˜ë¦¬ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            S3 íŒŒì¼ í‚¤ ë¦¬ìŠ¤íŠ¸

        Raises:
            AirflowSkipException: íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        """
        logger.info(f"ğŸ“‹ {target_date}ì˜ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹œì‘")

        s3_hook = S3Hook(aws_conn_id=CONN_ID)
        prefix = f"raw/api-1/dt={target_date}/"
        all_keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=prefix)
        # data.json íŒŒì¼ë§Œ í•„í„°ë§
        keys = [key for key in (all_keys or []) if key.endswith("data.json")]

        if not keys:
            logger.warning(f"âš ï¸ No raw files found for date: {target_date}")
            raise AirflowSkipException(f"No raw files found for date: {target_date}")

        logger.info(f"âœ… {len(keys)}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
        return keys

    @task
    def read_and_parse(file_keys: List[str]) -> List[Dict[str, Any]]:
        """JSON íŒŒì¼ì„ ì½ì–´ì„œ ê¸°ë³¸ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            file_keys: S3 íŒŒì¼ í‚¤ ë¦¬ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ê° ë ˆì½”ë“œëŠ” í’ˆëª©ì˜ ê°€ê²© ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬)

        Note:
            - dataê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°(ì—ëŸ¬ ì‘ë‹µ) ìŠ¤í‚µ
            - day1~day7: ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥ (ì˜ˆ: "ë‹¹ì¼ (11/01)", "1ì£¼ì¼ì „ (09/24)")
            - dpr1~dpr7: ê°€ê²© ì •ì œ í›„ ì €ì¥ (ì‰¼í‘œ ì œê±°, None ì²˜ë¦¬)
        """
        logger.info(f"ğŸ”„ {len(file_keys)}ê°œ íŒŒì¼ íŒŒì‹± ì‹œì‘")

        s3_hook = S3Hook(aws_conn_id=CONN_ID)

        all_records = []
        skipped_files = 0

        for i, key in enumerate(file_keys, 1):
            try:
                content_str = s3_hook.read_key(key=key, bucket_name=BUCKET_NAME)
                content = json.loads(content_str)

                metadata = extract_metadata_from_path(key)

                # data ì„¹ì…˜ì´ ë¦¬ìŠ¤íŠ¸(ì—ëŸ¬ ì‘ë‹µ)ì¸ ê²½ìš° ìŠ¤í‚µ
                data_section = content.get("data", {})
                if isinstance(data_section, list):
                    logger.debug(f"Skipping file with list data (code: {data_section}): {key}")
                    skipped_files += 1
                    continue

                items = data_section.get("item", [])
                if not isinstance(items, list):
                    items = [items]

                for item in items:
                    record = {
                        # ë©”íƒ€ë°ì´í„°
                        "res_dt": metadata["res_dt"],
                        "product_cls_cd": metadata["product_cls_cd"],
                        "category_cd": metadata["category_cd"],
                        "country_cd": metadata["country_cd"],
                        # í’ˆëª© ì •ë³´
                        "item_nm": str(item.get("item_name", "")).strip(),
                        "item_cd": str(item.get("item_code", "")).strip(),
                        "kind_nm": str(item.get("kind_name", "")).strip(),
                        "kind_cd": str(item.get("kind_code", "")).strip(),
                        "rank_nm": str(item.get("rank", "")).strip(),
                        "rank_cd": str(item.get("rank_code", "")).strip(),
                        "unit": str(item.get("unit", "")).strip(),
                        # ë‚ ì§œ ë¼ë²¨ (ë¬¸ìì—´ ê·¸ëŒ€ë¡œ: "ë‹¹ì¼ (10/01)", "1ì¼ì „ (09/30)" ë“±)
                        "base_dt": str(item.get("day1", "")).strip(),
                        "prev_1d_dt": str(item.get("day2", "")).strip(),
                        "prev_1w_dt": str(item.get("day3", "")).strip(),
                        "prev_2w_dt": str(item.get("day4", "")).strip(),
                        "prev_1m_dt": str(item.get("day5", "")).strip(),
                        "prev_1y_dt": str(item.get("day6", "")).strip(),
                        "avg_tp": str(item.get("day7", "")).strip(),
                        # ê°€ê²© ì •ë³´ (ì‰¼í‘œ ì œê±°, None ì²˜ë¦¬)
                        "base_pr": clean_price(item.get("dpr1", "")),
                        "prev_1d_pr": clean_price(item.get("dpr2", "")),
                        "prev_1w_pr": clean_price(item.get("dpr3", "")),
                        "prev_2w_pr": clean_price(item.get("dpr4", "")),
                        "prev_1m_pr": clean_price(item.get("dpr5", "")),
                        "prev_1y_pr": clean_price(item.get("dpr6", "")),
                        "avg_pr": clean_price(item.get("dpr7", "")),
                    }
                    all_records.append(record)

                if i % 10 == 0:
                    logger.info(f"   ì§„í–‰ë¥ : {i}/{len(file_keys)} ({i / len(file_keys) * 100:.1f}%)")

            except Exception as e:
                logger.exception(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {key} | ì—ëŸ¬: {e!s}")  # noqa: TRY401
                continue

        logger.info(f"âœ… {len(all_records):,}ê°œ ë ˆì½”ë“œ íŒŒì‹± ì™„ë£Œ (ìŠ¤í‚µëœ íŒŒì¼: {skipped_files}ê°œ)")
        return all_records

    @task
    def enrich_data(records: List[Dict[str, Any]]) -> pd.DataFrame:
        """ì½”ë“œ ë§¤í•‘ ë° íŒŒìƒ ì»¬ëŸ¼ ìƒì„±

        Args:
            records: ì›ë³¸ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            enrichmentëœ ë°ì´í„°í”„ë ˆì„

        ì²˜ë¦¬ ë‚´ìš©:
        - ì½”ë“œ â†’ ëª…ì¹­ ë§¤í•‘ (ìƒí’ˆë¶„ë¥˜, ì¹´í…Œê³ ë¦¬, ì§€ì—­)
        - ìš”ì¼ ì •ë³´ ì¶”ê°€ (weekday_num: 0~6, weekday_nm: ì›”~ì¼, weekend_yn)
        - ì£¼ì°¨ ì •ë³´ ì¶”ê°€ (week_of_year: ISO 8601 ê¸°ì¤€)
        - base_prì´ Noneì¸ ë ˆì½”ë“œ ì œê±°
        - res_dtë¥¼ date íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì‹œë¶„ì´ˆ ì—†ìŒ)
        """
        # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
        column_order = [
            # ë‚ ì§œ/ì‹œê°„ ì •ë³´
            "res_dt",
            "week_of_year",
            "weekday_num",
            "weekday_nm",
            "weekend_yn",
            # ìƒí’ˆ ë¶„ë¥˜
            "product_cls_cd",
            "product_cls_nm",
            "category_cd",
            "category_nm",
            # ì§€ì—­
            "country_cd",
            "country_nm",
            # í’ˆëª© ì •ë³´
            "item_nm",
            "item_cd",
            "kind_nm",
            "kind_cd",
            "rank_nm",
            "rank_cd",
            "unit",
            # ë‹¹ì¼ ì •ë³´
            "base_dt",
            "base_pr",
            # 1ì¼ì „
            "prev_1d_dt",
            "prev_1d_pr",
            # 1ì£¼ì¼ì „
            "prev_1w_dt",
            "prev_1w_pr",
            # 2ì£¼ì¼ì „
            "prev_2w_dt",
            "prev_2w_pr",
            # 1ê°œì›”ì „
            "prev_1m_dt",
            "prev_1m_pr",
            # 1ë…„ì „
            "prev_1y_dt",
            "prev_1y_pr",
            # í‰ë…„
            "avg_tp",
            "avg_pr",
        ]

        if not records:
            logger.warning("âš ï¸ ë¹ˆ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸")
            return pd.DataFrame(columns=column_order)

        logger.info(f"ğŸ·ï¸ {len(records):,}ê°œ ë ˆì½”ë“œ enrichment ì‹œì‘")

        df = pd.DataFrame(records)
        df["res_dt"] = pd.to_datetime(df["res_dt"])

        # ì½”ë“œ â†’ ëª…ì¹­ ë§¤í•‘
        product_cls_map = {"01": "ì†Œë§¤", "02": "ë„ë§¤"}
        category_map = {
            "100": "ì‹ëŸ‰ì‘ë¬¼",
            "200": "ì±„ì†Œë¥˜",
            "300": "íŠ¹ìš©ì‘ë¬¼",
            "400": "ê³¼ì¼ë¥˜",
            "500": "ì¶•ì‚°ë¬¼",
            "600": "ìˆ˜ì‚°ë¬¼",
        }
        country_map = {
            "1101": "ì„œìš¸",
            "2100": "ë¶€ì‚°",
            "2200": "ëŒ€êµ¬",
            "2401": "ê´‘ì£¼",
            "2501": "ëŒ€ì „",
            "all": "ì „ì²´ì§€ì—­",
        }

        df["product_cls_nm"] = df["product_cls_cd"].map(product_cls_map).fillna("ë¯¸ë¶„ë¥˜")
        df["category_nm"] = df["category_cd"].map(category_map).fillna("ë¯¸ë¶„ë¥˜")
        df["country_nm"] = df["country_cd"].map(country_map).fillna("ê¸°íƒ€")

        # ìš”ì¼ ì •ë³´ (pandas ê¸°ë³¸ ê°’: 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
        df["weekday_num"] = df["res_dt"].dt.dayofweek

        weekday_map = {0: "ì›”ìš”ì¼", 1: "í™”ìš”ì¼", 2: "ìˆ˜ìš”ì¼", 3: "ëª©ìš”ì¼", 4: "ê¸ˆìš”ì¼", 5: "í† ìš”ì¼", 6: "ì¼ìš”ì¼"}
        df["weekday_nm"] = df["weekday_num"].map(weekday_map)
        df["weekend_yn"] = df["weekday_num"].isin([5, 6])  # í† ìš”ì¼(5), ì¼ìš”ì¼(6)

        # ì£¼ì°¨ ì •ë³´ (ISO 8601 ê¸°ì¤€)
        df["week_of_year"] = df["res_dt"].dt.isocalendar().week.astype(np.int32)

        # í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ (base_prì´ ì—†ëŠ” ë ˆì½”ë“œ ì œê±°)
        initial_count = len(df)
        df = df.dropna(subset=["base_pr"])
        removed_count = initial_count - len(df)

        if removed_count > 0:
            logger.info(f"ğŸ§¹ base_pr NA ì œê±°: {removed_count:,}ê°œ ë ˆì½”ë“œ ì‚­ì œ")
        logger.info(f"âœ… Enrichment ì™„ë£Œ: {len(df):,}ê°œ ë ˆì½”ë“œ")

        # res_dtë¥¼ date íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì‹œë¶„ì´ˆ ì—†ëŠ” ë‚ ì§œë§Œ)
        df["res_dt"] = df["res_dt"].dt.date

        return df[column_order]

    @task
    def save_parquet(df: pd.DataFrame, target_date: str) -> Dict[str, Any]:
        """S3ì— ìµœì¢… Parquet ì €ì¥ (ë‚ ì§œë³„ ê°œë³„ íŒŒì¼)

        Args:
            df: ì €ì¥í•  ë°ì´í„°í”„ë ˆì„
            target_date: ì²˜ë¦¬ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬

        ì €ì¥ ì „ëµ:
        - ê²½ë¡œ: silver/api-1/year=YYYY/month=MM/data_YYYYMMDD.parquet
        - ë‚ ì§œë³„ë¡œ ë³„ë„ íŒŒì¼ ì €ì¥
        - ê°™ì€ ë‚ ì§œ ì¬ì²˜ë¦¬ ì‹œ í•´ë‹¹ íŒŒì¼ë§Œ ë®ì–´ì“°ê¸°
        - ë‹¤ë¥¸ ë‚ ì§œ ë°ì´í„°ëŠ” ì˜í–¥ ì—†ìŒ (ì•ˆì „)
        """
        if df.empty:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"date": target_date, "record_count": 0, "status": "no_data"}

        logger.info(f"ğŸ’¾ Parquet ì €ì¥ ì‹œì‘: {len(df):,}ê°œ ë ˆì½”ë“œ")

        s3_hook = S3Hook(aws_conn_id=CONN_ID)
        dt_obj = datetime.strptime(target_date, "%Y-%m-%d")

        # ë‚ ì§œë³„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
        path = f"silver/api-1/year={dt_obj.strftime('%Y')}/month={dt_obj.strftime('%m')}/"
        file_key = f"{path}data_{target_date.replace('-', '')}.parquet"

        # Parquetë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥
        buffer = BytesIO()
        df.to_parquet(buffer, engine="pyarrow", index=False)
        buffer.seek(0)

        s3_hook.load_bytes(bytes_data=buffer.getvalue(), key=file_key, bucket_name=BUCKET_NAME, replace=True)

        logger.info(f"âœ… Saved to: s3://{BUCKET_NAME}/{file_key}")
        logger.info(f"   Records: {len(df):,}ê°œ")
        logger.info("   Strategy: ë‚ ì§œë³„ ê°œë³„ íŒŒì¼ (ë‹¤ë¥¸ ë‚ ì§œ ë°ì´í„° ì•ˆì „)")

        # ìƒ˜í”Œ ë°ì´í„° ë¡œê¹… - ê¸°ë³¸ ì •ë³´
        sample_basic = df.head(3)[["res_dt", "week_of_year", "weekday_nm", "category_nm", "item_nm"]]
        logger.info(f"\nğŸ“‹ ì €ì¥ëœ ë°ì´í„° ìƒ˜í”Œ (ê¸°ë³¸ ì •ë³´):\n{sample_basic.to_string()}")

        # ìƒ˜í”Œ ë°ì´í„° ë¡œê¹… - ê°€ê²© ì •ë³´ (ë‚ ì§œ ë¼ë²¨ í¬í•¨)
        sample_price = df.head(3)[["item_nm", "base_dt", "base_pr", "prev_1d_dt", "prev_1d_pr"]]
        logger.info(f"\nğŸ’° ì €ì¥ëœ ë°ì´í„° ìƒ˜í”Œ (ê°€ê²© ì •ë³´):\n{sample_price.to_string()}")

        return {"date": target_date, "record_count": len(df), "file_key": file_key, "status": "success"}

    # --- DAG Flow ---
    # data_interval_start - 1ì¼ = ì „ì¼ ë°ì´í„° ì²˜ë¦¬
    target_date = "{{ (data_interval_start - macros.timedelta(days=1)).strftime('%Y-%m-%d') }}"

    file_keys = list_raw_files(target_date)
    raw_records = read_and_parse(file_keys)
    enriched_df = enrich_data(raw_records)
    save_parquet(enriched_df, target_date)


transform_api1_raw_to_silver()
