from datetime import datetime, timedelta
import logging
import re

from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from config.constants import BUCKET_NAME
from connection_utils import get_storage_conn_id
from metadata_loader_utils import MetadataLoader
import pandas as pd
from preprocessing import add_date_features, normalize_price, prepare_metadata
from s3_uploader_utils import read_json_from_s3, read_parquet_from_s3, upload_parquet_to_s3

logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
METADATA_KEY = "metadata/dim_product_no.csv"
ACTION_17 = "periodRetailProductList"
EXCLUDED_COUNTY_NAMES = ["í‰ê· ", "í‰ë…„"]
SILVER_PREFIX = "silver/api-17"

# ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
COLUMN_ORDER = [
    "res_dt",
    "week_of_year",
    "weekday_num",
    "weekday_nm",
    "weekend_yn",
    "product_no",
    "product_cls_cd",
    "product_cls_nm",
    "country_cd",
    "country_nm",
    "county_nm",
    "category_cd",
    "category_nm",
    "item_cd",
    "item_nm",
    "kind_cd",
    "kind_nm",
    "rank_cd",
    "rank_nm",
    "market_nm",
    "price",
    "year",
    "month",
]


def parse_file_path(file_path: str) -> dict:
    """íŒŒì¼ ê²½ë¡œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    result = {}

    product_cls_match = re.search(r"product_cls=(\w+)", file_path)
    result["product_cls_cd"] = product_cls_match.group(1) if product_cls_match else None

    country_match = re.search(r"country=(\w+)", file_path)
    result["country_cd"] = country_match.group(1) if country_match else None

    category_match = re.search(r"category=(\w+)", file_path)
    result["category_cd"] = category_match.group(1) if category_match else None

    item_match = re.search(r"item=(\w+)", file_path)
    result["item_cd"] = item_match.group(1) if item_match else None

    kind_match = re.search(r"kind=(\w+)", file_path)
    result["kind_cd"] = kind_match.group(1) if kind_match else None

    product_rank_match = re.search(r"product_rank=(\w+)", file_path)
    result["rank_cd"] = product_rank_match.group(1) if product_rank_match else None

    return result


def format_dataframe(df: pd.DataFrame, object_name: str) -> pd.DataFrame:
    """DataFrame í¬ë§·íŒ…"""
    path_info = parse_file_path(object_name)

    # ì§€ì—­ ì½”ë“œ ì—­ë§¤í•‘ (ì½”ë“œ -> ì´ë¦„)
    country_codes = MetadataLoader.get_country_codes()
    country_code_reverse = {v: k for k, v in country_codes.items()}

    path_info["country_nm"] = country_code_reverse.get(path_info["country_cd"], "ê¸°íƒ€")

    for key, value in path_info.items():
        df[key] = value

    def parse_regday(row: pd.Series) -> pd.Timestamp | None:
        if pd.isna(row.get("regday")) or row.get("regday") == "":
            return None
        try:
            regday_str = str(row["regday"]).strip()
            if "/" in regday_str:
                month, day = regday_str.split("/")
                year = str(row.get("yyyy", "2025"))
                date_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                return pd.to_datetime(date_str, format="%Y-%m-%d")
            else:
                return None
        except Exception:
            return None

    df["res_dt"] = df.apply(parse_regday, axis=1)

    # ë‚ ì§œ íŒŒìƒ ì»¬ëŸ¼ ì¶”ê°€
    df = add_date_features(df, "res_dt")

    # ê°€ê²© ì •ê·œí™”
    df["price_numeric"] = normalize_price(df["price"])

    # ì»¬ëŸ¼ ì •ë¦¬
    df = df.drop(columns=["price", "yyyy", "regday", "itemname", "kindname"])
    df = df.rename(columns={"price_numeric": "price", "countyname": "county_nm", "marketname": "market_nm"})

    return df


@dag(
    dag_id="silver_api17_transform_daily",
    start_date=datetime(2025, 12, 10),
    schedule=None,
    catchup=False,
    max_active_runs=1,
    default_args={"depends_on_past": False, "owner": "jiyeon_kim"},
    tags=["preprocessing", "api17"],
    description="KAMIS API17 Raw ë°ì´í„°ë¥¼ ì½ì–´ ì½”ë“œ ë§¤í•‘ ë° íŒŒìƒ ì»¬ëŸ¼ì„ ì¶”ê°€í•œ Silver ë°ì´í„°ë¡œ ë³€í™˜",
)
def silver_api17_transform_daily():
    """
    KAMIS API17 Raw â†’ Silver ë³€í™˜ DAG

    Returns:
        None
    """
    s3_conn_id = get_storage_conn_id()

    @task
    def extract_date_info(**context) -> dict:
        """
        Airflow ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²˜ë¦¬í•  ë‚ ì§œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            **context: Airflow ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ë‚ ì§œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        logical_date = context.get("logical_date") or context.get("data_interval_start")

        if logical_date is None:
            logger.error("logical_date ë˜ëŠ” data_interval_startë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logical_date = datetime.now()
        else:
            logical_date = logical_date.date()

        target_date_obj = logical_date - timedelta(days=10)
        target_date = target_date_obj.strftime("%Y-%m-%d")
        year = logical_date.year
        month = logical_date.month
        month_str = f"{month:02d}"

        logger.info(f"ğŸ“… Processing date: {target_date} (Year: {year}, Month: {month})")

        return {
            "target_date": target_date,
            "year": year,
            "month": month,
            "month_str": month_str,
        }

    @task
    def list_json_files(date_info: dict) -> list[str]:
        """
        S3ì—ì„œ ì²˜ë¦¬ ëŒ€ìƒ JSON íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            date_info: ë‚ ì§œ ì •ë³´ ë”•ì…”ë„ˆë¦¬

        Returns:
            JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        target_date = date_info["target_date"]
        hook = S3Hook(aws_conn_id=s3_conn_id)

        prefix = f"raw/api-17/dt={target_date}/"
        logger.info(f"ğŸ” Searching for files with prefix: {prefix}")

        try:
            keys = hook.list_keys(bucket_name=BUCKET_NAME, prefix=prefix)
            json_files = [key for key in keys if key.endswith(".json")]
            logger.info(f"âœ… Found {len(json_files)} JSON files")
        except Exception as e:
            logger.warning(f"âš ï¸ Error listing files: {e}")
            return []
        return json_files

    @task
    def process_json_files(json_files: list[str]) -> pd.DataFrame:
        """
        JSON íŒŒì¼ë“¤ì„ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            json_files: JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³€í™˜ëœ DataFrame
        """
        if not json_files:
            logger.warning("âš ï¸ No JSON files to process")
            return pd.DataFrame()

        all_dataframes = []

        for file_path in json_files:
            logger.info(f"ğŸ“„ Processing: {file_path}")
            data = read_json_from_s3(s3_key=file_path)

            if data and isinstance(data, list) and len(data) > 0:
                df = pd.DataFrame(data)
                df_formatted = format_dataframe(df, file_path)

                # í‰ê· /í‰ë…„ ë°ì´í„° ì œê±°
                if "county_nm" in df_formatted.columns:
                    df_formatted = df_formatted[~df_formatted["county_nm"].isin(EXCLUDED_COUNTY_NAMES)]

                all_dataframes.append(df_formatted)

        if not all_dataframes:
            logger.warning("âš ï¸ No valid data found")
            return pd.DataFrame()

        df_new = pd.concat(all_dataframes, ignore_index=True)
        logger.info(f"âœ… Processed {len(df_new):,} records from {len(json_files)} files")
        return df_new

    @task
    def merge_metadata(df: pd.DataFrame) -> pd.DataFrame:
        """
        ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            df: ë³‘í•©í•  DataFrame

        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ ë³‘í•©ëœ DataFrame
        """
        if df.empty:
            logger.warning("âš ï¸ Empty DataFrame, skipping metadata merge")
            return df

        hook = S3Hook(aws_conn_id=s3_conn_id)
        response = hook.get_key(key=METADATA_KEY, bucket_name=BUCKET_NAME)

        if response is None:
            logger.error(f"âŒ Metadata file not found: {METADATA_KEY}")
            return df

        meta_data = prepare_metadata(response.get()["Body"].read())

        merge_keys = ["product_cls_cd", "category_cd", "item_cd", "kind_cd", "rank_cd"]
        df = pd.merge(df, meta_data, on=merge_keys, how="left")
        df = df.drop(columns=["county_nm"], errors="ignore")

        logger.info(f"âœ… Metadata merged: {len(df):,} records")
        return df

    @task
    def merge_existing_data(df_new: pd.DataFrame, date_info: dict) -> pd.DataFrame:
        """
        ê¸°ì¡´ Silver ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            df_new: ìƒˆë¡œ ì²˜ë¦¬í•œ DataFrame
            date_info: ë‚ ì§œ ì •ë³´ ë”•ì…”ë„ˆë¦¬

        Returns:
            ë³‘í•©ëœ DataFrame
        """
        if df_new.empty:
            logger.warning("âš ï¸ Empty DataFrame, skipping merge")
            return df_new

        year = date_info["year"]
        month_str = date_info["month_str"]
        target_date = date_info["target_date"]

        parquet_key = f"{SILVER_PREFIX}/year={year}/month={month_str}/data.parquet"
        df_existing = read_parquet_from_s3(s3_key=parquet_key)

        if df_existing is not None:
            logger.info(f"ğŸ“Š Existing data: {len(df_existing):,} records")
            # ìƒˆ ë°ì´í„°ì˜ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì¡´ ë°ì´í„° ì œê±°
            df_existing = df_existing[df_existing["res_dt"] != target_date]
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            logger.info(f"âœ… Combined data: {len(df_combined):,} records")
        else:
            df_combined = df_new
            logger.info("i No existing parquet file, using new data only")

        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        existing_columns = [col for col in COLUMN_ORDER if col in df_combined.columns]
        remaining_columns = [col for col in df_combined.columns if col not in existing_columns]
        df_combined = df_combined[existing_columns + remaining_columns]

        return df_combined

    @task
    def upload_to_s3(df: pd.DataFrame, date_info: dict) -> None:
        """
        ìµœì¢… DataFrameì„ Parquet í˜•ì‹ìœ¼ë¡œ S3ì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            df: ì €ì¥í•  DataFrame
            date_info: ë‚ ì§œ ì •ë³´ ë”•ì…”ë„ˆë¦¬

        Returns:
            None
        """
        if df.empty:
            logger.warning("âš ï¸ Empty DataFrame, skipping upload")
            return

        year = date_info["year"]
        month_str = date_info["month_str"]
        target_date = date_info["target_date"]

        parquet_key = f"{SILVER_PREFIX}/year={year}/month={month_str}/data.parquet"

        upload_parquet_to_s3(
            df=df,
            s3_key=parquet_key,
        )
        logger.info(f"âœ… Silver transformation completed: {target_date} -> {parquet_key}")

    # DAG ì‹¤í–‰ íë¦„ ì •ì˜
    date_info = extract_date_info()
    json_files = list_json_files(date_info)
    df_processed = process_json_files(json_files)
    df_with_metadata = merge_metadata(df_processed)
    df_final = merge_existing_data(df_with_metadata, date_info)
    upload_to_s3(df_final, date_info)


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
silver_api17_transform_daily()
