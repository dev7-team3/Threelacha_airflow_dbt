from datetime import datetime, timedelta
import json
import logging
import os
from pathlib import Path

from airflow.models.dag import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.standard.operators.python import PythonOperator
from connection_utils import get_storage_conn_id
import dotenv
import requests

dotenv.load_dotenv()

logger = logging.getLogger(__name__)

S3_CONN_ID = get_storage_conn_id()
API_KEY = os.getenv("CERT_KEY")
ID = os.getenv("CERT_ID")

BUCKET_NAME = "team3-batch"
REQUEST_URL = "https://www.kamis.or.kr/service/price/xml.do?action=periodRetailProductList"

with Path.open(Path(__file__).parent.parent / "plugins" / "param_tree.json", "r", encoding="utf-8") as f:
    params_tree = json.load(f)

with Path.open(Path(__file__).parent.parent / "plugins" / "country_code.json", "r", encoding="utf-8") as f:
    country_code_mapping = json.load(f)


def set_category_product_variety_retail_code() -> dict:
    """ì¹´í…Œê³ ë¦¬, í’ˆëª©, í’ˆì¢…, íŒë§¤ì½”ë“œ ì •ë³´ë¥¼ ì„¤ì •

    Yields:
        Iterator[dict]: ì¹´í…Œê³ ë¦¬, í’ˆëª©, í’ˆì¢…, íŒë§¤ì½”ë“œ ì •ë³´
    """
    for category in params_tree:
        for product in params_tree[category]["products"]:
            for variety in params_tree[category]["products"][product]["varieties"]:
                for retail_code in params_tree[category]["products"][product]["varieties"][variety]["retail_codes"]:
                    yield {
                        "item_category_code": category,
                        "item_code": product,
                        "kind_code": variety,
                        "product_rank_code": retail_code,
                    }


def group_data_by_date(data: dict) -> dict[str, list] | None:
    """
    ë‚ ì§œë³„ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”

    Args:
        data (dict): API ì‘ë‹µ ë°ì´í„°

    Returns:
        dict[str, list] | None: ë‚ ì§œë³„ ë°ì´í„°. ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜
    """
    grouped = {}

    if data["data"]["error_code"] != "000":
        return None

    for item in data["data"]["item"]:
        # yyyy: "2025", regday: "12/17" -> "20251217"
        yyyy = item.get("yyyy", "")
        regday = item.get("regday", "")

        if yyyy and regday:
            # "12/17" -> "1217" (MM/DD -> MMDD)
            month_day = regday.replace("/", "-")
            date_str = f"{yyyy}-{month_day}"  # "2025-12-17"

            if date_str not in grouped:
                grouped[date_str] = []
            grouped[date_str].append(item)

    return grouped


def get_data(
    country_code: str,
    item_category_code: str,
    item_code: str,
    kind_code: str,
    product_rank_code: str,
    start_day: str,
    end_day: str,
) -> dict | None:
    """APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´

    Args:
        country_code (str): ë„ì‹œ ì½”ë“œ
        item_category_code (str): ì¹´í…Œê³ ë¦¬ ì½”ë“œ
        item_code (str): í’ˆëª© ì½”ë“œ
        kind_code (str): í’ˆì¢… ì½”ë“œ
        product_rank_code (str): íŒë§¤ì½”ë“œ
        start_day (str): ì‹œì‘ ë‚ ì§œ
        end_day (str): ì¢…ë£Œ ë‚ ì§œ

    Returns:
        dict | None: ë‚ ì§œë³„ ë°ì´í„°. ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜
    """
    url = f"{REQUEST_URL}&p_cert_key={API_KEY}&p_cert_id={ID}&p_returntype=json&p_startday={start_day}&p_endday={end_day}&p_countrycode={country_code}&p_convert_kg_yn=N&p_itemcategorycode={item_category_code}&p_itemcode={item_code}&p_kindcode={kind_code}&p_productrankcode={product_rank_code}"

    logger.info(f"ğŸ”„ Getting data for {url}")

    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        data = response.json()
        return group_data_by_date(data)
    except requests.exceptions.RequestException as e:
        raise Exception(f"Error: {e}") from e


def upload_data_to_s3(
    hook: S3Hook,
    country_code: str,
    date_data: dict[str, list],
    category_info: dict,
) -> str:
    """ë‚ ì§œë³„ ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ

    Args:
        hook (S3Hook): S3Hook ì¸ìŠ¤í„´ìŠ¤
        country_code (str): ë„ì‹œ ì½”ë“œ
        date_data (dict[str, list]): ë‚ ì§œë³„ ë°ì´í„°
        category_info (dict): ì¹´í…Œê³ ë¦¬, í’ˆëª©, í’ˆì¢…, íŒë§¤ì½”ë“œ ì •ë³´
    """
    # ê²½ë¡œ êµ¬ì„±: raw/api-17/dt=YYYYMMDD/product_cls=01/country=1101/category=100/item=111/kind=01/product_rank=04/data.json
    product_cls = "01"
    item_category_code = category_info["item_category_code"]
    item_code = category_info["item_code"]
    kind_code = category_info["kind_code"]
    product_rank_code = category_info["product_rank_code"]

    for date_str, data in date_data.items():
        key = (
            f"raw/api-17/dt={date_str}/"
            f"product_cls={product_cls}/country={country_code}/category={item_category_code}/"
            f"item={item_code}/kind={kind_code}/product_rank={product_rank_code}/data.json"
        )

        json_data = json.dumps(data, ensure_ascii=False)

        hook.load_string(
            string_data=json_data,
            key=key,
            bucket_name=BUCKET_NAME,
            replace=True,
        )
        logger.info(f"âœ… Uploaded data to {key}")


def get_data_by_country_code(country_code: str, **context) -> dict:
    """ë„ì‹œ ì½”ë“œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´

    Args:
        country_code (str): ë„ì‹œ ì½”ë“œ
        **context: ì»¨í…ìŠ¤íŠ¸

    Returns:
        dict: ë‚ ì§œë³„ ë°ì´í„°
    """
    logical_date = context.get("logical_date") or context.get("data_interval_start")

    if logical_date is None:
        raise ValueError("logical_date ë˜ëŠ” data_interval_startë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    start_day = (logical_date - timedelta(days=1)).strftime("%Y-%m-%d")
    end_day = logical_date.strftime("%Y-%m-%d")

    hook = S3Hook(aws_conn_id=S3_CONN_ID)

    for category in set_category_product_variety_retail_code():
        data_from_api = get_data(
            country_code=country_code,
            item_category_code=category["item_category_code"],
            item_code=category["item_code"],
            kind_code=category["kind_code"],
            product_rank_code=category["product_rank_code"],
            start_day=start_day,
            end_day=end_day,
        )

        if data_from_api is None:
            logger.warning(
                f"âŒ No data found for {country_code} {start_day} {end_day} {category['item_category_code']} {category['item_code']} {category['kind_code']} {category['product_rank_code']}"
            )
            continue

        upload_data_to_s3(
            hook=hook,
            country_code=country_code,
            date_data=data_from_api,
            category_info=category,
        )


with DAG(
    dag_id="raw_api17_collect_daily",
    start_date=datetime(2025, 12, 10),
    schedule="0 5 * * *",
    catchup=False,
    max_active_runs=1,
    default_args={
        "depends_on_past": False,
        "owner": "jiyeon_kim",
        "retries": 3,
        "retry_delay": timedelta(minutes=5),
    },
    tags=["api_ingestion"],
) as dag:
    for country_code in country_code_mapping.values():
        get_data_by_region_task = PythonOperator(
            task_id=f"get_data_by_region_{country_code}",
            python_callable=get_data_by_country_code,
            op_kwargs={
                "country_code": country_code,
            },
        )
